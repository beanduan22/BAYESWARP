**BayesWarp** is a principled framework for white-box testing of deep neural networks, leveraging Bayesian optimization and interpretability techniques to systematically uncover diverse model failures while keeping generated samples close to the original data distribution.

---

## ðŸš© Key Features

- **Principled Failure Discovery:** Region localization via GradCAM, Integrated Gradients, and SmoothGrad, focusing search on critical input areas.
- **Bayesian Optimization:** Adaptive, SVGP-accelerated perturbation search to expose diverse and realistic model failures.
- **Comprehensive Metrics:** Built-in evaluation metrics (NoF, DoF, Seed Coverage, FID) for rigorous comparison.
- **Flexible Model and Dataset Support:** Pre-defined pipelines for LeNet, ResNet, VGG; compatible with MNIST, CIFAR-10, and ImageNet-50.
---

## ðŸ’» Installation

pip install -r requirements.txt

---

## ðŸ“¦ Dataset Preparation

* **MNIST / CIFAR-10:**
  Run:

  ```bash
  cd data
  bash prepare_data.sh
  ```
* **ImageNet**
  Please organize your subset into `data/imagenet/train` and `data/imagenet/val` directories with only the selected 50 classes.

---

## ðŸš€ Quick Start

### Train a Model

# LeNet4/LeNet5 on MNIST
python bayeswarp/lenet4/train/train_lenet4.py
python bayeswarp/lenet5/train/train_lenet5.py

python bayeswarp/resnet18/train/train_resnet18.py
python bayeswarp/vgg16/train/train_vgg16.py

python bayeswarp/resnet50/train/train_resnet50.py
python  bayeswarp/vgg19/train/train_vgg19.py

### Run BayesWarp Testing

use the core.py in each model file

### Evaluate and Analyze Results

## ðŸ“Š Reproducing Experiments

* Training, testing,and evaluation are provided.

---
